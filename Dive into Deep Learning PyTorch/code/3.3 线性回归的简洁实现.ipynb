{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 线性回归的简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.1 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_input = 2 \n",
    "num_examples = 1000 \n",
    "\n",
    "true_w = [2,-3.4]\n",
    "true_b =4.2 \n",
    "\n",
    "# [1000,2]\n",
    "features = torch.tensor(np.random.normal(0,1, (num_examples, num_input)),\n",
    "                       dtype = torch.float32)\n",
    "\n",
    "features.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = true_w[0]*features[:,0]+ true_w[1]*features[:,1]+true_b\n",
    "labels+= torch.tensor(np.random.normal(0, 0.01, size = labels.size()), dtype= torch.float32)\n",
    "labels.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2626e+00,  4.4058e+00,  1.1486e+01,  5.0196e+00,  3.9994e+00,\n",
       "         7.5720e+00,  5.6822e+00,  5.0596e+00,  6.6707e+00,  7.5861e+00,\n",
       "         9.8118e+00,  3.5012e+00,  5.3696e+00,  9.7664e+00,  1.7827e+00,\n",
       "         1.0070e+01,  1.3953e+00,  3.2899e+00,  1.1453e+01,  1.5385e+01,\n",
       "         1.3400e+00,  1.8073e+00,  6.9806e+00,  7.6943e+00, -4.6515e+00,\n",
       "         2.8089e+00,  1.9710e+00,  4.7650e+00,  8.8328e+00,  5.4200e-02,\n",
       "        -3.8086e+00,  5.3170e+00,  4.8404e+00,  5.3249e+00,  7.6755e+00,\n",
       "         6.3386e+00, -2.3789e+00,  2.9974e+00, -3.0395e+00,  9.1863e+00,\n",
       "         5.6961e+00,  6.2492e+00,  4.1853e+00,  6.6389e+00,  3.6867e+00,\n",
       "         3.1361e+00,  6.9297e+00,  1.6425e+00,  3.8440e+00,  1.9943e+00,\n",
       "         4.1926e+00,  4.4000e+00,  8.2863e+00,  5.1585e+00, -2.6393e+00,\n",
       "         4.8699e+00,  3.1367e+00,  1.1313e+00,  1.1087e+01, -1.1336e-01,\n",
       "         3.1194e+00,  1.1694e+01, -1.5656e+00,  3.4128e+00,  2.8105e+00,\n",
       "        -7.1335e-01,  7.7286e+00,  4.5420e+00,  4.8058e+00,  6.9958e-01,\n",
       "         6.1118e+00,  3.1847e+00,  4.1344e+00,  1.0457e+01,  4.3861e+00,\n",
       "         8.6550e-01,  4.6200e-01, -4.0484e+00,  6.0435e+00,  5.6189e+00,\n",
       "        -9.8838e-01,  5.2475e-01,  7.3453e+00,  5.0892e+00,  9.8816e+00,\n",
       "         1.1369e+01, -2.5543e+00,  8.9480e+00, -2.8633e+00,  4.0579e+00,\n",
       "         2.9674e+00,  1.0173e+00, -3.3580e+00,  4.4682e+00,  5.4878e+00,\n",
       "         1.2177e+01, -2.8371e-01,  1.2324e+01, -4.6678e+00,  1.2443e+01,\n",
       "         4.0094e+00,  5.0840e+00,  8.3212e+00,  1.0406e+01,  5.2883e+00,\n",
       "         1.0164e+01,  3.6327e+00, -1.3319e+00,  3.7762e+00,  5.8865e+00,\n",
       "         6.1934e+00,  3.6797e+00,  5.9736e+00,  4.8263e+00,  5.1875e+00,\n",
       "         6.2168e+00,  1.1762e+00,  4.9570e+00,  9.0472e+00,  4.4135e+00,\n",
       "        -3.2106e+00,  2.9522e+00,  6.0938e+00,  1.0677e+01,  5.3860e+00,\n",
       "         4.0974e+00, -7.8429e-01,  3.2412e+00,  1.7366e+00, -3.5390e+00,\n",
       "         5.0875e+00,  6.6415e+00,  5.0802e+00,  1.0233e+01,  8.1691e+00,\n",
       "         3.6184e+00, -3.3693e+00,  4.8052e+00,  3.5151e-01, -3.3241e-01,\n",
       "        -2.4848e+00,  6.2981e-02,  7.1818e-01,  1.2025e+00, -2.6706e+00,\n",
       "         7.1854e+00,  8.9362e+00,  1.1423e+00,  1.1636e+01,  6.9925e+00,\n",
       "        -1.1946e+00,  5.8313e-02,  2.6046e+00,  7.5598e+00,  1.0295e+01,\n",
       "         8.3064e+00,  2.9665e+00,  8.2591e+00,  1.1040e+01,  3.7465e+00,\n",
       "         1.1991e+00,  1.0937e+01,  4.3723e+00,  1.0452e+01,  1.8342e+00,\n",
       "         4.1884e+00,  2.3466e+00,  5.1162e+00,  7.8626e+00,  2.8822e+00,\n",
       "         5.9139e+00,  5.1486e+00,  1.2039e+00,  6.2139e+00,  1.3813e+01,\n",
       "        -2.1085e+00,  1.0065e+01, -1.0727e-01,  9.5737e+00,  1.1593e+01,\n",
       "         6.3143e+00,  2.2912e+00,  3.5469e+00,  1.2136e+01,  5.0708e+00,\n",
       "         1.1862e+00,  8.5790e+00,  9.3412e+00,  3.1573e+00,  4.0021e+00,\n",
       "         3.3757e+00,  2.8026e+00,  8.5329e-01,  1.5002e+00,  4.0087e+00,\n",
       "         1.7053e+00,  1.1881e+00,  1.0797e+01,  2.3798e-01,  4.2755e+00,\n",
       "         3.5294e+00,  8.5738e+00,  6.9483e+00,  7.9880e+00,  8.2483e+00,\n",
       "        -5.4688e+00,  6.1345e+00,  8.8317e+00,  4.6832e+00,  9.7017e+00,\n",
       "         7.8263e+00,  7.0290e+00,  3.0618e+00,  1.3698e+00, -2.4843e-01,\n",
       "         7.5266e+00,  7.2117e+00,  2.9923e+00,  6.0594e+00,  4.4438e+00,\n",
       "         4.0006e+00,  7.6842e+00,  1.2970e+00,  3.1374e+00, -2.1939e+00,\n",
       "         7.5506e+00,  1.3813e+00, -1.6475e+00,  2.8459e+00,  8.6555e+00,\n",
       "         1.4344e+01,  7.9333e+00,  2.2549e+00,  1.6233e+00,  8.1155e+00,\n",
       "         3.0781e+00, -1.7480e+00,  1.2698e+00,  3.9471e+00,  1.5816e+01,\n",
       "         6.2179e+00,  6.0457e+00,  4.4376e+00,  1.1630e+01,  7.1015e+00,\n",
       "         9.1341e+00,  2.6848e+00,  7.7959e+00,  4.7252e+00,  4.7857e+00,\n",
       "         7.6248e+00,  1.3718e+00,  4.2665e-01,  5.0919e+00,  8.3983e+00,\n",
       "         5.9830e+00,  4.5592e+00,  7.9933e+00, -1.2744e+00,  2.0933e+00,\n",
       "         4.6800e+00,  9.3435e+00,  2.7990e+00,  5.0493e+00,  3.4870e+00,\n",
       "         6.7281e+00,  3.9755e+00,  4.6438e+00,  4.1978e+00,  7.3482e+00,\n",
       "         1.5169e+00,  4.5619e+00,  4.1044e-02,  3.8564e+00,  8.5008e+00,\n",
       "         8.4825e-01,  4.5590e+00,  4.2484e+00, -7.8567e+00,  3.7345e+00,\n",
       "         3.2813e+00,  4.9224e+00,  2.3027e+00,  7.2442e+00,  7.4252e+00,\n",
       "         5.4146e+00,  4.7604e+00,  4.8446e+00,  1.0739e+01,  8.1836e+00,\n",
       "         8.2704e+00,  7.1583e+00,  7.4275e+00,  3.9961e+00, -2.1410e+00,\n",
       "         5.6749e+00,  7.7145e+00,  7.2049e+00,  1.3631e-01,  1.0402e+00,\n",
       "         1.1656e+00, -3.6455e-03,  5.8907e+00,  1.3518e+01, -1.1365e+00,\n",
       "         8.9631e+00,  6.9301e+00, -1.7882e+00,  5.8860e+00,  7.1468e+00,\n",
       "         5.1842e+00,  4.2804e+00,  1.4515e+01,  4.6359e+00,  5.9073e+00,\n",
       "         1.7355e+00,  5.4750e+00,  2.9913e+00,  4.4575e+00,  7.9329e+00,\n",
       "         6.6237e+00,  7.3814e+00,  7.3535e+00,  5.4788e+00,  1.0350e+01,\n",
       "         3.2033e+00, -6.7559e-01,  1.7894e+00,  3.6122e+00,  1.0019e+01,\n",
       "         9.5473e-01,  3.8345e+00,  6.1860e+00,  9.4707e+00, -1.0797e+00,\n",
       "         7.9321e-01,  6.0668e+00,  8.1881e+00, -8.9208e-01,  5.8966e+00,\n",
       "         4.2543e+00,  9.5718e+00,  1.2376e+00,  3.7999e+00,  1.6919e+01,\n",
       "         4.3591e+00,  7.0994e+00,  9.8940e+00,  4.0218e+00, -6.8766e-01,\n",
       "        -4.0198e-01,  9.9892e+00,  6.6234e+00,  6.1160e+00,  9.0940e+00,\n",
       "        -2.6583e+00,  2.9450e+00,  2.5392e+00,  6.1135e+00,  9.5594e+00,\n",
       "        -3.7976e+00,  8.5028e+00, -3.8318e-01,  4.8272e+00,  2.2609e+00,\n",
       "         4.4121e+00,  9.3258e-01,  4.0970e+00,  9.0506e+00,  5.8111e+00,\n",
       "         2.2955e+00,  7.4074e+00,  4.1020e+00,  6.3631e+00,  6.3514e+00,\n",
       "         5.3722e+00,  7.7282e+00,  4.1092e+00,  2.9466e+00,  8.6223e+00,\n",
       "        -3.2519e+00,  8.6403e+00,  1.9872e+00, -5.5134e+00,  3.3488e+00,\n",
       "         7.2980e+00,  3.7758e+00,  6.3506e+00,  1.2964e+00,  1.3336e+00,\n",
       "        -2.1822e+00,  7.9877e+00,  4.6521e+00,  2.9222e+00, -1.3105e-01,\n",
       "         1.0745e+01,  1.3869e+01,  7.0727e+00,  7.5326e+00,  2.4560e-01,\n",
       "         7.6858e+00,  4.3748e+00, -3.2435e-01,  3.6874e+00,  5.6446e-01,\n",
       "         1.7711e+00,  3.0201e+00,  7.4213e+00,  1.2762e+00,  3.4211e+00,\n",
       "         5.7440e+00, -1.1838e+00,  6.7079e-01,  2.8742e+00,  7.6026e+00,\n",
       "         4.9802e+00,  3.3096e+00,  7.6681e+00, -1.4647e+00,  2.3680e+00,\n",
       "         6.4594e+00,  2.0466e+00, -5.7775e+00,  5.3679e+00,  5.2917e+00,\n",
       "         5.4394e+00,  5.6299e+00,  7.9000e+00, -2.8142e-01,  4.2204e+00,\n",
       "         1.2987e+00,  4.8754e+00,  7.6901e+00, -2.2904e+00,  6.7909e+00,\n",
       "         1.0858e+00,  4.1498e+00,  4.7941e+00,  1.7934e+00,  2.9741e+00,\n",
       "         9.7020e+00, -3.2283e-01,  4.2730e+00,  3.5651e+00,  1.7942e+00,\n",
       "         4.2560e+00,  2.6525e+00,  7.8262e+00,  1.1432e+01,  1.0625e+01,\n",
       "         4.3125e+00,  6.3361e+00,  6.3670e+00,  1.4140e+00,  4.7456e+00,\n",
       "         7.3733e+00,  6.0724e+00,  3.3251e+00,  1.7490e+00,  9.1351e+00,\n",
       "         3.4663e+00,  8.8878e+00,  6.3500e+00,  6.0503e+00, -1.8677e+00,\n",
       "         5.9990e+00,  1.3202e+01, -1.4270e+00,  6.6221e+00,  4.8691e+00,\n",
       "         4.6165e+00,  9.0371e-01, -2.8102e+00,  5.9596e+00,  7.6890e+00,\n",
       "         6.9192e-01,  8.8907e+00,  1.7305e+00,  6.9638e+00,  1.0525e+01,\n",
       "         5.8625e+00,  4.4621e+00,  2.3523e+00,  3.7905e+00,  5.2999e+00,\n",
       "         7.1245e+00,  1.1477e+00,  9.8238e+00,  2.5112e+00,  9.1560e+00,\n",
       "         9.0479e+00,  1.8585e+00,  5.8237e+00, -5.4586e+00,  4.5364e+00,\n",
       "         4.5709e+00,  5.5111e+00, -9.8610e-01,  9.0954e+00,  1.0349e+00,\n",
       "         4.0987e+00,  8.3245e+00,  3.1786e+00,  5.9012e+00,  2.0168e+00,\n",
       "        -1.1613e+00,  3.7108e+00,  1.0797e+01,  5.1760e-01,  5.3312e+00,\n",
       "         2.2001e+00,  4.5560e+00,  5.7877e+00, -1.1691e-01,  6.2449e+00,\n",
       "         7.3894e+00,  3.6425e+00,  7.3539e+00,  1.3768e+01,  5.3115e+00,\n",
       "        -2.8081e+00,  5.5312e+00,  1.2505e+00,  9.5965e-01, -9.1292e+00,\n",
       "         2.1290e+00,  4.9044e+00,  1.7288e+00,  1.0583e+00,  5.0469e+00,\n",
       "         9.4554e+00,  3.3141e+00,  1.8096e+00,  4.9346e+00,  3.0710e+00,\n",
       "        -1.5328e+00,  9.0257e+00,  7.2222e+00,  4.4557e+00,  7.9891e+00,\n",
       "         4.4385e+00, -3.3877e+00,  2.4961e+00, -1.6242e+00, -3.2858e+00,\n",
       "        -1.9878e+00,  5.1401e+00,  6.4237e+00,  6.2312e+00,  7.7801e+00,\n",
       "         7.2252e+00,  8.1662e+00,  4.8360e+00,  1.0268e+01,  7.9584e+00,\n",
       "         2.3894e+00,  8.3477e+00,  8.3917e+00,  3.3838e+00,  4.8103e+00,\n",
       "         1.0328e+01, -7.2929e-01,  4.7566e+00,  6.0663e+00,  9.1505e+00,\n",
       "        -5.4755e+00,  5.5502e+00,  2.8672e-01,  7.3898e+00,  6.4886e+00,\n",
       "         8.0714e+00,  5.5545e+00, -8.8807e-02,  5.1807e+00,  2.6217e+00,\n",
       "         9.5745e+00, -3.6559e+00,  5.9435e-02,  9.4826e+00, -3.0692e-01,\n",
       "         3.1602e+00,  5.9926e+00,  1.7625e+00,  7.5048e+00,  5.1207e+00,\n",
       "         2.0954e+00,  3.2108e+00,  8.1356e+00,  4.2144e+00,  5.9333e+00,\n",
       "        -4.1297e-01,  3.7999e+00,  1.3860e+01,  9.3848e+00, -3.2552e+00,\n",
       "         9.7350e-01, -8.2108e-01,  1.6570e+00,  1.1991e+01,  4.0652e+00,\n",
       "         1.7970e+00,  8.7026e+00, -9.1531e-02,  3.4886e+00,  6.5557e+00,\n",
       "         4.1716e+00,  5.5935e+00,  1.8386e+00,  2.9966e+00,  6.3403e+00,\n",
       "         8.1532e+00,  9.3626e+00,  9.9451e+00,  8.9999e+00,  7.3576e+00,\n",
       "         7.2104e+00,  5.9520e+00,  2.2109e+00,  3.7734e+00,  1.0554e+01,\n",
       "         5.2056e+00,  6.8070e+00, -1.2808e+00,  6.4623e+00,  8.3606e+00,\n",
       "         8.2954e+00,  1.1857e+01,  6.5031e+00,  1.8988e+00,  4.2479e+00,\n",
       "        -1.3837e+00, -5.1542e+00,  1.2812e+01,  8.8048e-01,  1.0936e+01,\n",
       "         9.2732e+00,  3.1176e+00,  5.2507e+00,  6.0421e+00,  6.0557e+00,\n",
       "         1.9191e+00,  4.9495e+00, -6.7022e-01,  5.7671e+00,  3.2450e+00,\n",
       "         4.7939e+00,  8.1334e+00,  4.6060e+00,  6.0537e+00,  3.7792e+00,\n",
       "         7.1568e+00,  8.6126e+00,  5.0890e+00,  2.2033e+00,  6.8875e+00,\n",
       "         5.1187e+00,  5.6542e+00,  9.1288e+00,  6.6442e+00,  1.2407e+01,\n",
       "         2.1038e+00,  4.7762e+00,  6.5040e-01, -9.1593e-01,  8.5323e+00,\n",
       "         2.9212e+00,  7.0696e+00,  4.1878e-01,  6.4508e+00,  5.3576e+00,\n",
       "         8.0231e+00,  3.7233e+00,  3.1833e+00,  1.2518e+01,  1.3176e+00,\n",
       "         6.9353e+00,  9.0613e+00,  2.8527e+00,  1.0191e+01,  3.1104e+00,\n",
       "         2.2969e+00,  7.5911e+00,  4.1198e+00,  2.5387e+00,  3.8293e+00,\n",
       "         3.0011e+00,  6.6473e+00, -6.7900e+00,  2.6946e+00,  5.4042e+00,\n",
       "        -2.7918e+00,  7.2137e+00,  4.6897e+00,  1.2255e+01,  2.3655e+00,\n",
       "         5.4799e+00,  4.3130e+00,  2.1193e+00,  6.5410e+00,  9.6922e+00,\n",
       "         6.5071e+00, -4.3082e+00,  9.0971e+00,  7.1491e+00, -2.0065e+00,\n",
       "        -1.8286e-01,  2.9896e+00,  5.9701e+00,  4.4685e+00,  1.6708e+00,\n",
       "         4.2483e+00,  2.0047e+00,  5.0876e+00, -4.0072e-01,  1.4482e+00,\n",
       "         9.4005e+00,  4.8392e+00, -2.1684e+00,  6.5258e+00,  5.4387e+00,\n",
       "         1.0931e+01,  5.6135e+00, -2.9923e+00,  6.0162e-02,  4.5468e+00,\n",
       "         1.5179e-01,  3.3214e+00,  7.7337e+00,  5.8257e+00,  4.2813e+00,\n",
       "         1.5487e+01,  3.2992e+00,  8.7936e+00,  6.5410e+00,  7.0375e+00,\n",
       "         9.2889e+00,  3.0484e-01,  6.1593e+00,  8.4468e+00,  8.7673e+00,\n",
       "        -2.1689e+00,  3.5403e+00,  2.9721e+00,  1.9439e+00,  2.0218e+00,\n",
       "        -2.4205e+00,  1.0602e+01,  4.8938e+00,  3.6812e-01, -7.6213e-01,\n",
       "         2.2292e+00, -4.8674e+00,  4.9952e+00,  6.5322e+00,  4.5746e+00,\n",
       "         7.8307e+00,  6.6766e+00, -2.0077e+00,  1.0336e+00,  8.1040e+00,\n",
       "         5.4036e+00,  5.9209e+00,  9.7575e+00,  3.4626e-01,  4.4998e+00,\n",
       "         7.5962e+00,  1.4681e+00,  5.1575e+00,  7.3339e-01,  4.6252e+00,\n",
       "         5.4300e+00,  7.7157e-01,  7.1709e+00,  1.1056e+01,  6.9654e+00,\n",
       "         7.5368e-01,  5.3829e+00, -2.7750e+00, -5.1255e+00,  1.1842e+01,\n",
       "         4.5530e+00,  1.1405e+00, -1.8265e+00, -1.5635e+00, -1.6352e-01,\n",
       "         9.0318e-01,  4.4685e+00, -1.0080e+00,  4.2021e+00,  2.1961e+00,\n",
       "         5.6110e+00,  8.8967e-01, -2.8604e+00,  4.7469e+00, -4.4142e+00,\n",
       "         6.7712e+00,  7.3831e+00,  7.4941e+00,  1.2213e+00,  4.5747e+00,\n",
       "         5.8514e+00,  5.2111e+00, -3.2851e+00,  1.7670e+00,  3.5945e+00,\n",
       "         8.3579e+00,  3.0751e+00,  3.5086e+00,  1.1311e+01,  4.6221e+00,\n",
       "         7.1454e+00, -2.0791e+00,  8.3686e+00,  6.8860e+00,  3.6798e-01,\n",
       "         1.3123e+01, -2.8535e+00,  4.0388e+00,  1.0262e+01, -3.9410e-01,\n",
       "         8.0236e+00,  2.0870e+00,  4.8971e+00,  1.0739e+01,  4.5554e+00,\n",
       "         5.3642e+00,  7.9364e-01,  4.4397e+00,  2.8087e+00,  7.2282e+00,\n",
       "         3.2263e+00,  7.3089e+00,  2.6893e+00,  4.3646e+00,  5.4932e+00,\n",
       "         1.2436e+01, -5.4955e-01,  2.6542e+00,  1.2164e+00,  5.7118e+00,\n",
       "         1.0173e+00,  1.1726e+00,  1.9971e+00, -7.5665e-01,  3.8969e+00,\n",
       "         7.7060e+00,  3.8002e+00,  3.7695e+00, -3.2409e+00,  1.8440e+00,\n",
       "        -2.6459e+00,  8.6583e+00,  7.3402e+00,  2.3111e+00, -7.7565e-01,\n",
       "         2.1568e+00,  2.0037e+00, -1.6763e-01,  6.5446e+00,  3.2747e+00,\n",
       "         1.4661e+00,  4.8853e+00,  1.0380e+01,  5.9415e+00,  5.0213e+00,\n",
       "         1.1005e+01,  1.0134e+01,  3.4311e+00,  3.8855e+00,  4.7856e+00,\n",
       "        -1.0020e+00,  3.8278e+00, -3.5281e+00,  9.4856e+00, -2.1359e-01,\n",
       "         7.7974e-01,  3.2655e+00, -8.2857e-01,  6.3188e+00,  4.5433e+00,\n",
       "         2.3571e+00,  1.2097e+01,  5.5954e+00,  7.2972e+00,  5.8988e+00,\n",
       "         2.8841e+00,  5.9921e+00,  2.3688e+00,  8.2275e+00,  1.6633e+00,\n",
       "         2.5007e+00,  9.8759e+00,  7.1501e+00,  1.1031e+00, -1.1734e+00,\n",
       "         3.5999e+00, -2.9760e+00,  1.1663e+01,  5.7920e+00,  3.1943e+00,\n",
       "         3.1281e+00,  3.8514e+00,  5.9793e+00,  1.8562e+00,  9.4835e+00,\n",
       "        -7.5819e-01,  5.4868e+00,  5.3285e+00,  7.4140e+00,  1.1137e+01,\n",
       "         5.1442e+00, -6.9560e-02,  2.0034e+00,  8.1359e+00,  6.2560e+00,\n",
       "         5.2151e+00,  7.4940e+00,  8.1999e+00,  6.3039e+00,  7.2195e+00,\n",
       "        -5.3641e+00,  3.8607e+00,  1.2311e+01,  8.2417e+00,  8.3563e+00,\n",
       "         1.9283e+00,  4.7617e+00, -7.2525e-01,  8.5418e+00,  1.4056e+01,\n",
       "         3.9491e+00,  1.8186e+00,  3.0424e-01,  7.1286e+00,  8.9347e+00,\n",
       "         1.5221e+00,  4.1646e+00,  7.4734e+00, -1.0506e+00,  3.5754e+00,\n",
       "         3.2884e+00,  2.0494e+00,  2.0763e+00,  8.6451e+00,  7.6446e+00,\n",
       "         1.1381e+01,  6.7913e+00,  4.2825e+00, -2.6671e+00,  2.0969e+00,\n",
       "         1.2478e+01,  1.0835e+00,  1.7022e+00, -2.1278e+00,  1.1310e+01,\n",
       "        -3.7747e+00,  2.0421e+00,  2.2831e+00, -3.1442e-01,  2.0311e+00,\n",
       "         5.8127e+00,  6.7862e+00,  1.4626e+01,  1.0722e+01,  2.1299e+00,\n",
       "         8.3744e+00,  1.3994e+01,  2.7399e+00,  8.2149e+00,  4.3606e+00,\n",
       "         3.9652e+00,  3.9461e+00, -1.4141e+00,  4.4401e+00,  3.0745e+00,\n",
       "         3.5994e+00,  2.1219e+00,  5.2481e+00, -2.3774e+00,  3.8716e-01,\n",
       "         1.1657e+00,  9.6125e-01,  4.4124e+00,  6.5380e+00,  5.0421e+00,\n",
       "         2.7306e+00, -1.4327e+00,  9.8957e+00,  4.2235e+00,  2.6776e+00,\n",
       "         4.5744e+00, -4.0082e+00,  1.1700e+00, -1.1203e+00,  2.5164e+00,\n",
       "         2.4346e+00, -4.2923e-01, -2.2927e+00,  1.1643e+01,  5.0749e+00])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =10 \n",
    "# features [1000,2], laabs: [1000]\n",
    "# # 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# # 随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7043, -0.5162],\n",
      "        [ 0.0744, -0.2027],\n",
      "        [ 0.6107,  0.5866],\n",
      "        [ 1.9397,  0.2799],\n",
      "        [-0.0595, -1.2625],\n",
      "        [-1.3042, -1.1817],\n",
      "        [-0.8565, -0.3980],\n",
      "        [-0.7218, -0.4118],\n",
      "        [ 0.1065,  2.0282],\n",
      "        [-0.1131, -0.1738]]) tensor([ 2.5392,  5.0493,  3.4128,  7.1245,  8.3606,  5.6189,  3.8440,  4.1498,\n",
      "        -2.4848,  4.5590])\n",
      "###########\n",
      "torch.Size([10, 2]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for x , y in data_iter:\n",
    "    print(x,y)\n",
    "    print(\"###########\")\n",
    "    print(x.shape, y.shape) # torch.Size([10, 2]) torch.Size([10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearNet(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = LinearNet(num_input)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0374,  0.3280]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3147], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0374,  0.3280]], requires_grad=True)\n",
      "#########################\n",
      "linear.bias\n",
      "Parameter containing:\n",
      "tensor([-0.3147], requires_grad=True)\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print(\"#########################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearNet(\n",
       "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.LinearNet"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "注意 要进行下一步\n",
    "net[0] 这样的操作 \n",
    "直接 调用 net[0] 会报错\n",
    "需要做的是 sequencial it \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "nn.Linear(num_input,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0015,  0.6964]], requires_grad=True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0931], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.4 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net[0].weight, mean=0, std=0.01)\n",
    "init.constant_(net[0].bias, val=0)  # 也可以直接修改bias的data: net[0].bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.5 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.6 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr =0.001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 2]), torch.Size([10]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.5392,  5.0493,  3.4128,  7.1245,  8.3606,  5.6189,  3.8440,  4.1498,\n",
       "        -2.4848,  4.5590])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5392],\n",
       "        [ 5.0493],\n",
       "        [ 3.4128],\n",
       "        [ 7.1245],\n",
       "        [ 8.3606],\n",
       "        [ 5.6189],\n",
       "        [ 3.8440],\n",
       "        [ 4.1498],\n",
       "        [-2.4848],\n",
       "        [ 4.5590]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.7 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 19.923719 \n",
      "epoch: 2, loss: 10.414871 \n",
      "epoch: 3, loss: 7.089870 \n",
      "epoch: 4, loss: 4.951368 \n",
      "epoch: 5, loss: 2.818110 \n",
      "epoch: 6, loss: 2.257079 \n",
      "epoch: 7, loss: 1.049172 \n",
      "epoch: 8, loss: 0.743617 \n",
      "epoch: 9, loss: 1.030025 \n",
      "epoch: 10, loss: 0.808908 \n",
      "epoch: 11, loss: 0.494111 \n",
      "epoch: 12, loss: 0.181955 \n",
      "epoch: 13, loss: 0.197541 \n",
      "epoch: 14, loss: 0.115160 \n",
      "epoch: 15, loss: 0.088433 \n",
      "epoch: 16, loss: 0.060374 \n",
      "epoch: 17, loss: 0.028628 \n",
      "epoch: 18, loss: 0.011631 \n",
      "epoch: 19, loss: 0.016844 \n",
      "epoch: 20, loss: 0.012737 \n",
      "epoch: 21, loss: 0.004408 \n",
      "epoch: 22, loss: 0.006156 \n",
      "epoch: 23, loss: 0.001819 \n",
      "epoch: 24, loss: 0.001377 \n",
      "epoch: 25, loss: 0.000733 \n",
      "epoch: 26, loss: 0.001241 \n",
      "epoch: 27, loss: 0.000347 \n",
      "epoch: 28, loss: 0.001066 \n",
      "epoch: 29, loss: 0.000389 \n",
      "epoch: 30, loss: 0.000338 \n",
      "epoch: 31, loss: 0.000202 \n",
      "epoch: 32, loss: 0.000153 \n",
      "epoch: 33, loss: 0.000281 \n",
      "epoch: 34, loss: 0.000201 \n",
      "epoch: 35, loss: 0.000098 \n",
      "epoch: 36, loss: 0.000172 \n",
      "epoch: 37, loss: 0.000118 \n",
      "epoch: 38, loss: 0.000133 \n",
      "epoch: 39, loss: 0.000069 \n",
      "epoch: 40, loss: 0.000157 \n",
      "epoch: 41, loss: 0.000071 \n",
      "epoch: 42, loss: 0.000135 \n",
      "epoch: 43, loss: 0.000056 \n",
      "epoch: 44, loss: 0.000029 \n",
      "epoch: 45, loss: 0.000093 \n",
      "epoch: 46, loss: 0.000094 \n",
      "epoch: 47, loss: 0.000144 \n",
      "epoch: 48, loss: 0.000063 \n",
      "epoch: 49, loss: 0.000077 \n",
      "epoch: 50, loss: 0.000058 \n",
      "epoch: 51, loss: 0.000060 \n",
      "epoch: 52, loss: 0.000176 \n",
      "epoch: 53, loss: 0.000064 \n",
      "epoch: 54, loss: 0.000072 \n",
      "epoch: 55, loss: 0.000067 \n",
      "epoch: 56, loss: 0.000123 \n",
      "epoch: 57, loss: 0.000070 \n",
      "epoch: 58, loss: 0.000172 \n",
      "epoch: 59, loss: 0.000137 \n",
      "epoch: 60, loss: 0.000042 \n",
      "epoch: 61, loss: 0.000047 \n",
      "epoch: 62, loss: 0.000177 \n",
      "epoch: 63, loss: 0.000092 \n",
      "epoch: 64, loss: 0.000114 \n",
      "epoch: 65, loss: 0.000091 \n",
      "epoch: 66, loss: 0.000160 \n",
      "epoch: 67, loss: 0.000098 \n",
      "epoch: 68, loss: 0.000059 \n",
      "epoch: 69, loss: 0.000189 \n",
      "epoch: 70, loss: 0.000102 \n",
      "epoch: 71, loss: 0.000051 \n",
      "epoch: 72, loss: 0.000092 \n",
      "epoch: 73, loss: 0.000042 \n",
      "epoch: 74, loss: 0.000059 \n",
      "epoch: 75, loss: 0.000062 \n",
      "epoch: 76, loss: 0.000060 \n",
      "epoch: 77, loss: 0.000093 \n",
      "epoch: 78, loss: 0.000125 \n",
      "epoch: 79, loss: 0.000230 \n",
      "epoch: 80, loss: 0.000066 \n",
      "epoch: 81, loss: 0.000110 \n",
      "epoch: 82, loss: 0.000113 \n",
      "epoch: 83, loss: 0.000180 \n",
      "epoch: 84, loss: 0.000100 \n",
      "epoch: 85, loss: 0.000101 \n",
      "epoch: 86, loss: 0.000098 \n",
      "epoch: 87, loss: 0.000110 \n",
      "epoch: 88, loss: 0.000079 \n",
      "epoch: 89, loss: 0.000086 \n",
      "epoch: 90, loss: 0.000093 \n",
      "epoch: 91, loss: 0.000116 \n",
      "epoch: 92, loss: 0.000074 \n",
      "epoch: 93, loss: 0.000112 \n",
      "epoch: 94, loss: 0.000104 \n",
      "epoch: 95, loss: 0.000111 \n",
      "epoch: 96, loss: 0.000079 \n",
      "epoch: 97, loss: 0.000123 \n",
      "epoch: 98, loss: 0.000065 \n",
      "epoch: 99, loss: 0.000156 \n",
      "epoch: 100, loss: 0.000057 \n",
      "epoch: 101, loss: 0.000110 \n",
      "epoch: 102, loss: 0.000173 \n",
      "epoch: 103, loss: 0.000067 \n",
      "epoch: 104, loss: 0.000128 \n",
      "epoch: 105, loss: 0.000071 \n",
      "epoch: 106, loss: 0.000060 \n",
      "epoch: 107, loss: 0.000058 \n",
      "epoch: 108, loss: 0.000112 \n",
      "epoch: 109, loss: 0.000072 \n",
      "epoch: 110, loss: 0.000090 \n",
      "epoch: 111, loss: 0.000054 \n",
      "epoch: 112, loss: 0.000108 \n",
      "epoch: 113, loss: 0.000113 \n",
      "epoch: 114, loss: 0.000116 \n",
      "epoch: 115, loss: 0.000128 \n",
      "epoch: 116, loss: 0.000081 \n",
      "epoch: 117, loss: 0.000133 \n",
      "epoch: 118, loss: 0.000144 \n",
      "epoch: 119, loss: 0.000249 \n",
      "epoch: 120, loss: 0.000111 \n",
      "epoch: 121, loss: 0.000178 \n",
      "epoch: 122, loss: 0.000058 \n",
      "epoch: 123, loss: 0.000137 \n",
      "epoch: 124, loss: 0.000155 \n",
      "epoch: 125, loss: 0.000077 \n",
      "epoch: 126, loss: 0.000086 \n",
      "epoch: 127, loss: 0.000086 \n",
      "epoch: 128, loss: 0.000073 \n",
      "epoch: 129, loss: 0.000149 \n",
      "epoch: 130, loss: 0.000117 \n",
      "epoch: 131, loss: 0.000132 \n",
      "epoch: 132, loss: 0.000071 \n",
      "epoch: 133, loss: 0.000119 \n",
      "epoch: 134, loss: 0.000105 \n",
      "epoch: 135, loss: 0.000083 \n",
      "epoch: 136, loss: 0.000121 \n",
      "epoch: 137, loss: 0.000088 \n",
      "epoch: 138, loss: 0.000051 \n",
      "epoch: 139, loss: 0.000028 \n",
      "epoch: 140, loss: 0.000122 \n",
      "epoch: 141, loss: 0.000088 \n",
      "epoch: 142, loss: 0.000119 \n",
      "epoch: 143, loss: 0.000071 \n",
      "epoch: 144, loss: 0.000053 \n",
      "epoch: 145, loss: 0.000152 \n",
      "epoch: 146, loss: 0.000200 \n",
      "epoch: 147, loss: 0.000083 \n",
      "epoch: 148, loss: 0.000098 \n",
      "epoch: 149, loss: 0.000105 \n",
      "epoch: 150, loss: 0.000104 \n",
      "epoch: 151, loss: 0.000060 \n",
      "epoch: 152, loss: 0.000073 \n",
      "epoch: 153, loss: 0.000111 \n",
      "epoch: 154, loss: 0.000049 \n",
      "epoch: 155, loss: 0.000119 \n",
      "epoch: 156, loss: 0.000051 \n",
      "epoch: 157, loss: 0.000093 \n",
      "epoch: 158, loss: 0.000022 \n",
      "epoch: 159, loss: 0.000079 \n",
      "epoch: 160, loss: 0.000209 \n",
      "epoch: 161, loss: 0.000124 \n",
      "epoch: 162, loss: 0.000125 \n",
      "epoch: 163, loss: 0.000039 \n",
      "epoch: 164, loss: 0.000088 \n",
      "epoch: 165, loss: 0.000108 \n",
      "epoch: 166, loss: 0.000045 \n",
      "epoch: 167, loss: 0.000128 \n",
      "epoch: 168, loss: 0.000096 \n",
      "epoch: 169, loss: 0.000109 \n",
      "epoch: 170, loss: 0.000159 \n",
      "epoch: 171, loss: 0.000127 \n",
      "epoch: 172, loss: 0.000129 \n",
      "epoch: 173, loss: 0.000033 \n",
      "epoch: 174, loss: 0.000067 \n",
      "epoch: 175, loss: 0.000206 \n",
      "epoch: 176, loss: 0.000078 \n",
      "epoch: 177, loss: 0.000087 \n",
      "epoch: 178, loss: 0.000046 \n",
      "epoch: 179, loss: 0.000038 \n",
      "epoch: 180, loss: 0.000179 \n",
      "epoch: 181, loss: 0.000075 \n",
      "epoch: 182, loss: 0.000092 \n",
      "epoch: 183, loss: 0.000125 \n",
      "epoch: 184, loss: 0.000048 \n",
      "epoch: 185, loss: 0.000151 \n",
      "epoch: 186, loss: 0.000119 \n",
      "epoch: 187, loss: 0.000064 \n",
      "epoch: 188, loss: 0.000075 \n",
      "epoch: 189, loss: 0.000090 \n",
      "epoch: 190, loss: 0.000109 \n",
      "epoch: 191, loss: 0.000114 \n",
      "epoch: 192, loss: 0.000075 \n",
      "epoch: 193, loss: 0.000025 \n",
      "epoch: 194, loss: 0.000143 \n",
      "epoch: 195, loss: 0.000185 \n",
      "epoch: 196, loss: 0.000149 \n",
      "epoch: 197, loss: 0.000150 \n",
      "epoch: 198, loss: 0.000107 \n",
      "epoch: 199, loss: 0.000141 \n",
      "epoch: 200, loss: 0.000051 \n",
      "epoch: 201, loss: 0.000044 \n",
      "epoch: 202, loss: 0.000067 \n",
      "epoch: 203, loss: 0.000119 \n",
      "epoch: 204, loss: 0.000096 \n",
      "epoch: 205, loss: 0.000140 \n",
      "epoch: 206, loss: 0.000104 \n",
      "epoch: 207, loss: 0.000098 \n",
      "epoch: 208, loss: 0.000071 \n",
      "epoch: 209, loss: 0.000099 \n",
      "epoch: 210, loss: 0.000108 \n",
      "epoch: 211, loss: 0.000149 \n",
      "epoch: 212, loss: 0.000120 \n",
      "epoch: 213, loss: 0.000082 \n",
      "epoch: 214, loss: 0.000116 \n",
      "epoch: 215, loss: 0.000081 \n",
      "epoch: 216, loss: 0.000043 \n",
      "epoch: 217, loss: 0.000157 \n",
      "epoch: 218, loss: 0.000121 \n",
      "epoch: 219, loss: 0.000097 \n",
      "epoch: 220, loss: 0.000091 \n",
      "epoch: 221, loss: 0.000097 \n",
      "epoch: 222, loss: 0.000067 \n",
      "epoch: 223, loss: 0.000053 \n",
      "epoch: 224, loss: 0.000084 \n",
      "epoch: 225, loss: 0.000070 \n",
      "epoch: 226, loss: 0.000206 \n",
      "epoch: 227, loss: 0.000119 \n",
      "epoch: 228, loss: 0.000110 \n",
      "epoch: 229, loss: 0.000096 \n",
      "epoch: 230, loss: 0.000096 \n",
      "epoch: 231, loss: 0.000121 \n",
      "epoch: 232, loss: 0.000166 \n",
      "epoch: 233, loss: 0.000138 \n",
      "epoch: 234, loss: 0.000126 \n",
      "epoch: 235, loss: 0.000077 \n",
      "epoch: 236, loss: 0.000067 \n",
      "epoch: 237, loss: 0.000055 \n",
      "epoch: 238, loss: 0.000133 \n",
      "epoch: 239, loss: 0.000066 \n",
      "epoch: 240, loss: 0.000125 \n",
      "epoch: 241, loss: 0.000133 \n",
      "epoch: 242, loss: 0.000135 \n",
      "epoch: 243, loss: 0.000086 \n",
      "epoch: 244, loss: 0.000024 \n",
      "epoch: 245, loss: 0.000031 \n",
      "epoch: 246, loss: 0.000133 \n",
      "epoch: 247, loss: 0.000139 \n",
      "epoch: 248, loss: 0.000058 \n",
      "epoch: 249, loss: 0.000056 \n",
      "epoch: 250, loss: 0.000101 \n",
      "epoch: 251, loss: 0.000044 \n",
      "epoch: 252, loss: 0.000053 \n",
      "epoch: 253, loss: 0.000104 \n",
      "epoch: 254, loss: 0.000073 \n",
      "epoch: 255, loss: 0.000111 \n",
      "epoch: 256, loss: 0.000099 \n",
      "epoch: 257, loss: 0.000101 \n",
      "epoch: 258, loss: 0.000073 \n",
      "epoch: 259, loss: 0.000082 \n",
      "epoch: 260, loss: 0.000087 \n",
      "epoch: 261, loss: 0.000140 \n",
      "epoch: 262, loss: 0.000126 \n",
      "epoch: 263, loss: 0.000039 \n",
      "epoch: 264, loss: 0.000092 \n",
      "epoch: 265, loss: 0.000093 \n",
      "epoch: 266, loss: 0.000154 \n",
      "epoch: 267, loss: 0.000167 \n",
      "epoch: 268, loss: 0.000023 \n",
      "epoch: 269, loss: 0.000179 \n",
      "epoch: 270, loss: 0.000108 \n",
      "epoch: 271, loss: 0.000086 \n",
      "epoch: 272, loss: 0.000078 \n",
      "epoch: 273, loss: 0.000104 \n",
      "epoch: 274, loss: 0.000124 \n",
      "epoch: 275, loss: 0.000082 \n",
      "epoch: 276, loss: 0.000046 \n",
      "epoch: 277, loss: 0.000098 \n",
      "epoch: 278, loss: 0.000160 \n",
      "epoch: 279, loss: 0.000132 \n",
      "epoch: 280, loss: 0.000132 \n",
      "epoch: 281, loss: 0.000073 \n",
      "epoch: 282, loss: 0.000059 \n",
      "epoch: 283, loss: 0.000042 \n",
      "epoch: 284, loss: 0.000080 \n",
      "epoch: 285, loss: 0.000057 \n",
      "epoch: 286, loss: 0.000112 \n",
      "epoch: 287, loss: 0.000096 \n",
      "epoch: 288, loss: 0.000086 \n",
      "epoch: 289, loss: 0.000153 \n",
      "epoch: 290, loss: 0.000064 \n",
      "epoch: 291, loss: 0.000064 \n",
      "epoch: 292, loss: 0.000038 \n",
      "epoch: 293, loss: 0.000136 \n",
      "epoch: 294, loss: 0.000068 \n",
      "epoch: 295, loss: 0.000149 \n",
      "epoch: 296, loss: 0.000122 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 297, loss: 0.000132 \n",
      "epoch: 298, loss: 0.000077 \n",
      "epoch: 299, loss: 0.000140 \n",
      "epoch: 300, loss: 0.000114 \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300 \n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    for x, y in data_iter:\n",
    "        output = net(x) # [10,2]->[10,1]\n",
    "        logits = loss(output, y.view(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        logits.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"epoch: %d, loss: %f \"% (epoch, logits.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2, out_features=1, bias=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = net[0]\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, -3.4], Parameter containing:\n",
       " tensor([[ 2.0002, -3.3999]], requires_grad=True))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w, dense.weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.2, Parameter containing:\n",
       " tensor([4.1996], requires_grad=True))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b, dense.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
